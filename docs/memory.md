# Memory

## ConversationBufferMemory

이 메모리는 메시지를 저장한 다음 변수에 메시지를 추출할 수 있게 해줍니다.
먼저 문자열로 추출할 수 있습니다.

### 장점

- 구현이 간단하고 직관적입니다.
- 모든 대화 기록을 보존하여 컨텍스트를 완벽하게 유지할 수 있습니다.
- 대화의 전체 흐름을 파악하기 쉽습니다.

### 단점

- 메모리 크기가 너무 커질 수 있습니다.
- 메모리 크기를 제한하는 것은 model의 성능에 영향을 줄 수 있습니다.
- 토큰 제한에 쉽게 도달할 수 있습니다.
- 오래된 대화도 계속 메모리에 유지되어 불필요한 컨텍스트를 제공할 수 있습니다.

## ConversationBufferWindowMemory

- `ConversationBufferWindowMemory` 는 시간이 지남에 따라 대화의 상호작용 목록을 유지합니다.
- 이때, `ConversationBufferWindowMemory` 는 모든 대화내용을 활용하는 것이 아닌 **최근 K개** 의 상호작용만 사용합니다.
- 여기서 K는 대화 턴(turns)의 수를 의미합니다. 1턴은 사용자의 질문과 AI의 응답을 포함합니다.
  - 예: K=2인 경우, 가장 최근 2개의 대화 턴(4개의 메시지: 2개의 사용자 입력과 2개의 AI 응답)만 유지됩니다.

이는 버퍼가 너무 커지지 않도록 가장 최근 상호작용의 슬라이딩 창을 유지하는 데 유용할 수 있습니다.

### 장점

- 메모리 사용량을 효율적으로 관리할 수 있습니다.
- 최근 대화에 집중하여 더 관련성 높은 응답을 생성할 수 있습니다.
- 토큰 제한을 쉽게 관리할 수 있습니다.
- K값을 조절하여 컨텍스트 크기를 유연하게 조정할 수 있습니다.

### 단점

- 오래된 대화 컨텍스트가 완전히 손실될 수 있습니다.
- 중요한 이전 정보가 window 크기를 벗어나면 접근할 수 없습니다.
- K값 설정이 너무 작으면 중요한 컨텍스트를 놓칠 수 있습니다.
- K값 설정이 너무 크면 ConversationBufferMemory와 같은 문제가 발생할 수 있습니다.

## ConversationTokenBufferMemory

`ConversationTokenBufferMemory` 는 최근 대화의 히스토리를 버퍼를 메모리에 보관하고, 대화의 개수가 아닌 **토큰 길이** 를 사용하여 대화내용을 플러시(flush)할 시기를 결정합니다.

> 💡 **플러시(flush)란?**
>
> - 메모리에서 오래된 데이터를 제거하는 작업을 의미합니다.
> - 토큰 길이가 지정된 제한을 초과할 때, 가장 오래된 대화부터 제거하여 메모리를 정리합니다.
> - 이를 통해 메모리 사용량을 제한된 크기 내로 유지할 수 있습니다.

### 장점

- 토큰 제한을 정확하게 제어할 수 있습니다.
- 메모리 사용량을 효율적으로 관리할 수 있습니다.
- 긴 응답이 있는 경우에도 유연하게 대응할 수 있습니다.
- 토큰 기반 가격 정책에 최적화된 메모리 관리가 가능합니다.

### 단점

- 토큰 계산 오버헤드가 있을 수 있습니다.
- 중요한 컨텍스트가 토큰 제한으로 인해 손실될 수 있습니다.
- 대화의 논리적 단위(턴)와 관계없이 컨텍스트가 잘릴 수 있습니다.
- 토큰 수 설정이 너무 작으면 대화의 연속성이 떨어질 수 있습니다.

## ConversationEntityMemory

Entity 메모리는 대화에서 특정 Entity에 대한 주어진 사실을 기억합니다.

Entity 메모리는 Entity에 대한 정보를 추출하고(LLM 사용) 시간이 지남에 따라 해당 Entity에 대한 지식을 축적합니다(역시 LLM 사용).

> 💡 **Entity란?**
>
> - Entity는 대화에서 중요한 개체를 나타내는 개념입니다.
> - 예를 들어, 사람, 장소, 물건 등이 Entity가 될 수 있습니다.

### 장점

- 특정 Entity에 대한 정보를 체계적으로 추적하고 유지할 수 있습니다.
- 대화 맥락에서 중요한 개체들에 대한 정보를 선별적으로 저장합니다.
- 이전 대화에서 언급된 Entity 정보를 효과적으로 재사용할 수 있습니다.
- 장기적인 대화에서 일관된 Entity 정보 참조가 가능합니다.

### 단점

- LLM을 사용하여 Entity를 추출하고 관리하므로 추가적인 비용이 발생합니다.
- Entity 추출 과정에서 오류가 발생할 수 있습니다.
- 모든 대화 내용이 아닌 Entity 관련 정보만 저장되어 전체 맥락이 손실될 수 있습니다.
- 복잡한 관계나 문맥적 뉘앙스를 완벽하게 캡처하기 어려울 수 있습니다.

## ConversationKGMemory (knowledge graph memory)

지식 그래프의 힘을 활용하여 정보를 저장하고 불러최니다.

이를 통해 model이 서로 다른 개체 간의 관계를 이해하는 데 도움을 주고, 복잡한 연결망과 역사적 맥락을 기반으로 대응하는 능력을 향상시킵니다.

### 장점

- Entity 간의 복잡한 관계를 구조화된 형태로 저장할 수 있습니다.
- 연관된 정보를 쉽게 탐색하고 추론할 수 있습니다.
- 정보의 맥락과 관계를 유지하면서 장기 기억이 가능합니다.
- 새로운 관계를 동적으로 추가하고 업데이트할 수 있습니다.

### 단점

- 지식 그래프 구축과 유지에 많은 컴퓨팅 resource가 필요합니다.
- 복잡한 관계 추출 과정에서 정확도가 떨어질 수 있습니다.
- 구현과 관리가 다른 메모리 유형보다 복잡합니다.
- 그래프 구조가 커질수록 검색 및 업데이트 성능이 저하될 수 있습니다.

## ConversationSummaryMemory (おすすめ)

이제 조금 더 복잡한 메모리 유형인 `ConversationSummaryMemory` 를 사용하는 방법을 살펴 보겠습니다.
이 유형의 메모리는 시간 경과에 따른 **대화의 요약** 을 생성합니다. 이는 시간 경과에 따른 대화의 정보를 압축하는 데 유용할 수 있습니다.
대화 요약 메모리는 대화가 진행되는 동안 대화를 요약하고 **현재 요약을 메모리에 저장** 합니다.
그런 다음 이 메모리를 사용하여 지금까지의 대화 요약을 프롬프트/체인에 삽입할 수 있습니다.
이 메모리는 과거 메시지 기록을 프롬프트에 그대로 보관하면 토큰을 너무 많이 차지할 수 있는 긴 대화에 가장 유용합니다.

- 필자는 이 메모리를 사용하는것을 좋아한다.
  - ConversationEntityMemory, ConversationKGMemory 보다 detail 한 대답을 얻을 수 있었다.

### 장점

- 긴 대화를 효율적으로 압축하여 토큰 사용량을 줄일 수 있습니다.
- 오래된 대화의 핵심 내용을 유지하면서도 메모리를 효율적으로 관리할 수 있습니다.
- 전체 대화의 맥락을 잃지 않고 요약된 형태로 보존할 수 있습니다.
- 장기간의 대화에서 특히 유용합니다.

### 단점

- 요약 과정에서 중요한 세부 정보가 손실될 수 있습니다.
- 요약을 생성하기 위해 추가적인 LLM 호출이 필요하므로 비용이 증가합니다.
- 실시간 요약 생성으로 인한 지연이 발생할 수 있습니다.
- 요약의 품질이 LLM의 성능에 크게 의존합니다.

## ConversationSummaryBufferMemory (おすすめ)

`ConversationSummaryBufferMemory` 는 두 가지 아이디어를 결합한 것입니다.
최근 대화내용의 버퍼를 메모리에 유지하되, 이전 대화내용을 완전히 플러시(flush)하지 않고 요약으로 컴파일하여 두 가지를 모두 사용합니다.
대화내용을 플러시할 시기를 결정하기 위해 상호작용의 개수가 아닌 **토큰 길이** 를 사용합니다.

### 장점

- 최근 대화는 상세하게 유지하면서 오래된 대화는 요약본으로 보존할 수 있습니다.
- 토큰 제한을 효과적으로 관리하면서도 전체 컨텍스트를 유지할 수 있습니다.
- 버퍼와 요약을 결합하여 더 균형 잡힌 메모리 관리가 가능합니다.
- 장기 대화에서도 중요한 정보의 손실을 최소화할 수 있습니다.

### 단점

- 구현이 더 복잡하고 메모리 관리 로직이 복잡해집니다.
- 버퍼와 요약 모두를 유지해야 하므로 메모리 오버헤드가 증가할 수 있습니다.
- 요약 생성을 위한 추가적인 LLM 호출로 인한 비용과 지연이 발생합니다.
- 토큰 길이와 요약 타이밍의 적절한 균형을 찾기 어려울 수 있습니다.

## VectorStoreRetrieverMemory

`VectorStoreRetrieverMemory` 는 vector 스토어에 메모리를 저장하고 호출될 때마다 가장 '눈에 띄는' 상위 K개의 문서를 쿼리합니다.

이는 대화내용의 순서를 **명시적으로 추적하지 않는다는 점** 에서 다른 대부분의 메모리 클래스와 다릅니다.

### 장점

- 의미적 유사성을 기반으로 관련 정보를 효율적으로 검색할 수 있습니다.
- 대화 순서에 구애받지 않고 컨텍스트를 유연하게 활용할 수 있습니다.
- 대규모 대화 기록에서도 빠른 검색이 가능합니다.
- 새로운 정보를 쉽게 추가하고 업데이트할 수 있습니다.

### 단점

- vector 저장소 구축과 유지에 추가적인 infra가 필요합니다.
- embedding 생성에 따른 추가 비용이 발생합니다.
- 시간적 순서나 인과관계를 고려하지 않을 수 있습니다.
- 검색 결과의 품질이 embedding의 품질에 크게 의존합니다.

---

## 사용 케이스별 추천 메모리

### 1. 짧은 대화 (5턴 이내)

- **추천: ConversationBufferMemory**
  - 간단하고 직관적인 구현
  - 모든 컨텍스트를 완벽하게 유지
  - 대화가 짧아 메모리 부하가 적음

### 2. 중간 길이의 대화 (5-15턴)

- **추천: ConversationBufferWindowMemory**
  - K값을 조절하여 최근 대화에 집중
  - 메모리 사용량을 효율적으로 관리
  - 적절한 컨텍스트 유지와 성능의 균형

### 3. 긴 대화 (15턴 이상)

- **1순위: ConversationSummaryBufferMemory**
  - 최근 대화는 상세히, 과거 대화는 요약으로 유지
  - 전체 맥락을 잃지 않으면서 효율적인 메모리 관리
- **2순위: ConversationSummaryMemory**
  - 전체 대화를 요약하여 핵심 내용 유지
  - 토큰 사용량 최적화

### 4. 특정 개체 중심의 대화

- **추천: ConversationEntityMemory**
  - 주요 개체에 대한 정보를 체계적으로 추적
  - 개체 중심의 장기 기억 유지
  - 예: 고객 서비스, 개인화된 추천 시스템

### 5. 복잡한 관계를 다루는 대화

- **추천: ConversationKGMemory**
  - 개체 간의 관계를 그래프 형태로 저장
  - 복잡한 연관 정보 추적
  - 예: 제품 관계 분석, 소셜 네트워크 분석

### 6. 대규모 지식 기반 대화

- **추천: VectorStoreRetrieverMemory**
  - 대량의 정보에서 관련 컨텍스트 검색
  - 의미 기반 검색으로 정확한 정보 제공
  - 예: 문서 기반 QA, 기술 지원

### 7. 토큰 사용량 엄격 제한 케이스

- **추천: ConversationTokenBufferMemory**
  - 정확한 토큰 제어 가능
  - 비용 효율적인 메모리 관리
  - 예: 엄격한 비용 제한이 있는 프로덕션 환경

> 💡 **일반적인 추천**
>
> - 대부분의 일반적인 사용 케이스에서는 `ConversationSummaryBufferMemory`가 가장 균형 잡힌 선택입니다.
> - 단순한 구현이 필요한 경우 `ConversationBufferWindowMemory`로 시작하는 것이 좋습니다.
> - 특별한 요구사항이 없다면 복잡한 메모리 타입(Entity, KG)은 피하는 것이 좋습니다.

---
